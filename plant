#!/bin/bash

mkdir -p certs

cat << EOF > ~/.cndtrc
org {
	o  Stark and Wayne
	ou StarkAndWayne
	l  Buffalo
	st NY
	c USA
}
EOF

FQDN=build-man.sw.cafe
POD_CIDR=10.202.32.0/19
SERVICE_CIDR=10.202.16.0/20

IP=$(ip addr show dev eth0 |grep inet |head -n1|awk '{print $2}'|sed -e 's/\/[0-9]\+$//')
HOST=$(echo $FQDN|sed -e 's/\..*$//')
DOMAIN=$(echo $FQDN|sed -e "s/$HOST\.//")
KUBE_IP=$(echo $SERVICE_CIDR|sed -e 's/\.[0-9]\+\/[0-9]\+$/\.1/')
DNS_IP=$(echo $SERVICE_CIDR|sed -e 's/\.[0-9]\+\/[0-9]\+$/\.53/')

cd certs

conductor gen both $FQDN -i 127.0.0.1 -i $IP -i $KUBE_IP -d $HOST
conductor gen user system:kube-controller-manager -o system:kube-controller-manager
conductor gen user system:kube-scheduler -o system:kube-scheduler
conductor gen user system:kube-proxy -o system:node-proxier
conductor gen both system:node:$HOST -o system:nodes -d $FQDN -i $IP -d $HOST
conductor gen server service-account
conductor gen server git.$DOMAIN
conductor gen user admin -o system:masters

sudo mkdir -p /etc/kubernetes/{manifests,certs,etcd,api,controller,scheduler}

sudo cp ca_chain.pem                      /etc/kubernetes/certs/ca.chain.pem
sudo cp system\:node\:$HOST.fullchain.pem /etc/kubernetes/certs/cert.fullchain.pem
sudo cp system\:node\:$HOST.key.pem       /etc/kubernetes/certs/cert.key.pem

sudo cp ca_chain.pem             /etc/kubernetes/api/ca.chain.pem
sudo cp $FQDN.fullchain.pem      /etc/kubernetes/api/cert.fullchain.pem
sudo cp $FQDN.key.pem            /etc/kubernetes/api/cert.key.pem
sudo cp service-account.cert.pem /etc/kubernetes/api/

sudo cp ca_chain.pem        /etc/kubernetes/etcd/ca.chain.pem
sudo cp $FQDN.fullchain.pem /etc/kubernetes/etcd/cert.fullchain.pem
sudo cp $FQDN.key.pem       /etc/kubernetes/etcd/cert.key.pem

sudo cp ca_chain.pem                                  /etc/kubernetes/controller/ca.chain.pem
sudo cp system\:kube-controller-manager.key.pem       /etc/kubernetes/controller/cert.key.pem
sudo cp system\:kube-controller-manager.fullchain.pem /etc/kubernetes/controller/cert.fullchain.pem
sudo cp intermediate_cert.pem                         /etc/kubernetes/controller/intermediate.cert.pem
sudo cp intermediate_key.pem                          /etc/kubernetes/controller/intermediate.key.pem
sudo cp service-account.key.pem                       /etc/kubernetes/controller/

sudo cp ca_chain.pem                         /etc/kubernetes/scheduler/ca.chain.pem
sudo cp system\:kube-scheduler.key.pem       /etc/kubernetes/scheduler/cert.key.pem
sudo cp system\:kube-scheduler.fullchain.pem /etc/kubernetes/scheduler/cert.fullchain.pem
sudo chown root:root -R /etc/kubernetes 


sudo su -c "cat << EOF > /etc/kubernetes/kubelet.yml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
evictionHard:
  memory.available:  \"200Mi\"
tlsCertFile:       /etc/kubernetes/certs/cert.fullchain.pem
tlsPrivateKeyFile: /etc/kubernetes/certs/cert.key.pem
staticPodPath:     /etc/kubernetes/manifests
address: ${IP}
CgroupDriver:      cgroupfs
CgroupRoot:        /
clusterDomain: ${DOMAIN}
clusterDNS:
  - ${DNS_IP}
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/certs/ca.chain.pem
authorization:
  mode: Webhook
KubeletAuthentication:
  KubeletX509Authentication:
    ClientCAFile: /etc/kubernetes/certs/ca.chain.pem
EOF"

sudo su -c "cat << EOF > /etc/kubernetes/kubeconfig.yml
apiVersion: v1
kind: Config
clusters:
- name: sw
  cluster:
    server: https://${IP}:6443
    certificate-authority: /etc/kubernetes/certs/ca.chain.pem
users:
- name: kubelet
  user:
    client-certificate: /etc/kubernetes/certs/cert.fullchain.pem
    client-key:         /etc/kubernetes/certs/cert.key.pem
contexts:
- context:
    cluster: sw
    user: kubelet
  name: sw-context
current-context: sw-context
EOF"
sudo su -c "cat << EOF > /etc/kubernetes/controller/kubeconfig.yml
apiVersion: v1
kind: Config
clusters:
- name: sw
  cluster:
    server: https://${IP}:6443
    certificate-authority: /etc/kubernetes/controller/ca.chain.pem
users:
- name: kubelet
  user:
    client-certificate: /etc/kubernetes/controller/cert.fullchain.pem
    client-key:         /etc/kubernetes/controller/cert.key.pem
contexts:
- context:
    cluster: sw
    user: kubelet
  name: sw-context
current-context: sw-context
EOF"
sudo su -c "cat << EOF > /etc/kubernetes/scheduler/kubeconfig.yml
apiVersion: v1
kind: Config
clusters:
- name: sw
  cluster:
    server: https://${IP}:6443
    certificate-authority: /etc/kubernetes/scheduler/ca.chain.pem
users:
- name: kubelet
  user:
    client-certificate: /etc/kubernetes/scheduler/cert.fullchain.pem
    client-key:         /etc/kubernetes/scheduler/cert.key.pem
contexts:
- context:
    cluster: sw
    user: kubelet
  name: sw-context
current-context: sw-context
EOF"

sudo su -c "cat << EOF > /etc/kubernetes/manifests/etcd.yml
---
apiVersion: v1
kind: Pod
metadata:
  name: etcd
  namespace: kube-system
spec:
  containers:
  - image: docker.io/whisperos/etcd:3.3.10
    command:
    - /etcd
    args:
    - --name=${FQDN}
    - --data-dir=/var/lib/etcd
    - --trusted-ca-file=/etc/kubernetes/etcd/ca.chain.pem
    - --peer-trusted-ca-file=/etc/kubernetes/etcd/ca.chain.pem
    - --client-cert-auth
    - --cert-file=/etc/kubernetes/etcd/cert.fullchain.pem
    - --key-file=/etc/kubernetes/etcd/cert.key.pem
    - --peer-cert-file=/etc/kubernetes/etcd/cert.fullchain.pem
    - --peer-key-file=/etc/kubernetes/etcd/cert.key.pem
    - --enable-v2=false
    - --peer-auto-tls=false
    - --initial-cluster-state=new
    - --initial-cluster-token='boot-kube-etcd-cluster'
    - --initial-cluster
    - '${FQDN}=https://${IP}:2380'
    - --listen-peer-urls
    - https://${IP}:2380
    - --listen-client-urls
    - http://127.0.0.1:2379,https://${IP}:2379
    - --initial-advertise-peer-urls
    - https://${FQDN}:2380
    - --advertise-client-urls
    - http://127.0.0.1:2379,https://${IP}:2379,https://${FQDN}:2379
    #livenessProbe:
    #  httpGet:
    #    host: 127.0.0.1
    #    path: /health
    #    port: 2379
    #    schema: HTTP
    #  initialDelaySeconds: 5
    #  timeoutSeconds: 15
    name: etcd
    ports:
    - containerPort: 2380
      hostPort: 2380
      name: serverport
    - containerPort: 2379
      hostPort: 2379
      name: clientport
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: lib
      readOnly: false
    - mountPath: /var/log/etcd.log
      name: log
      readOnly: false
    - mountPath: /etc/kubernetes/etcd
      name: etc
      readOnly: true
  hostNetwork: true
  volumes:
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: lib
  - hostPath:
      path: /var/log/etcd.log
      type: FileOrCreate
    name: log
  - hostPath:
      path: /etc/kubernetes/etcd
    name: etc
EOF"
sudo su -c "cat << EOF > /etc/kubernetes/manifests/control-plane.yml
---
apiVersion: v1
kind: Pod
metadata:
  name: control-plane
  namespace: kube-system
spec:
  hostNetwork: true
  volumes:
  - hostPath:
      path: /etc/kubernetes/api
    name: etc-api
  - hostPath:
      path: /etc/kubernetes/scheduler
    name: etc-scheduler
  - hostPath:
      path: /etc/kubernetes/controller
    name: etc-controller
  containers:
  - image: docker.io/whisperos/kube-apiserver:1.12.3
    name: kube-apiserver
    command:
    - /kube-apiserver
    args:
    - --etcd-servers=https://${IP}:2379
    - --etcd-cafile=/etc/kubernetes/api/ca.chain.pem
    - --etcd-certfile=/etc/kubernetes/api/cert.fullchain.pem
    - --etcd-keyfile=/etc/kubernetes/api/cert.key.pem
    - --tls-cert-file=/etc/kubernetes/api/cert.fullchain.pem
    - --tls-private-key-file=/etc/kubernetes/api/cert.key.pem
    - --kubelet-certificate-authority=/etc/kubernetes/api/ca.chain.pem
    - --kubelet-client-certificate=/etc/kubernetes/api/cert.fullchain.pem
    - --kubelet-client-key=/etc/kubernetes/api/cert.key.pem
    - --cert-dir=/etc/kubernetes/api
    - --cloud-provider=external
    - --allow-privileged
    - --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota
    - --client-ca-file=/etc/kubernetes/api/ca.chain.pem
    - --external-hostname=${FQDN}
    - --insecure-port=0
    - --service-cluster-ip-range=${SERVICE_CIDR}
    - --bind-address=${IP}
    - --advertise-address=${IP}
    - --apiserver-count=1
    - --endpoint-reconciler-type=master-count
    - --authorization-mode=Node,RBAC
    - --enable-aggregator-routing=true
    - --service-account-key-file=/etc/kubernetes/api/service-account.cert.pem
    - --requestheader-client-ca-file=/etc/kubernetes/api/ca.chain.pem
    - --requestheader-allowed-names=aggregator,metrics-server,admin,system:kube-proxy,system:kube-controller-manager,system:kube-scheduler
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    livenessProbe:
      httpGet:
        host: \"${IP}\"
        path: /healthz
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 15
      timeoutSeconds: 15
    ports:
    - containerPort: 6443
      hostPort: 6443
      name: https
    volumeMounts:
    - mountPath: /etc/kubernetes/api
      name: etc-api
      readOnly: true
  - name: kube-controller-manager
    image: docker.io/whisperos/kube-controller-manager:1.12.3
    command:
    - /kube-controller-manager
    args:
    - --kubeconfig=/etc/kubernetes/controller/kubeconfig.yml
    - --cluster-name=sw
    - --tls-cert-file=/etc/kubernetes/controller/cert.fullchain.pem
    - --tls-private-key-file=/etc/kubernetes/controller/cert.key.pem
    - --cluster-signing-cert-file=/etc/kubernetes/controller/intermediate.cert.pem
    - --cluster-signing-key-file=/etc/kubernetes/controller/intermediate.key.pem
    - --root-ca-file=/etc/kubernetes/controller/ca.chain.pem
    - --service-account-private-key-file=/etc/kubernetes/controller/service-account.key.pem
    - --cloud-provider=external
    - --allocate-node-cidrs
    - --cluster-cidr=${POD_CIDR}
    - --service-cluster-ip-range=${SERVICE_CIDR}
    - --use-service-account-credentials=true
    livenessProbe:
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10252
        scheme: HTTP
      initialDelaySeconds: 15
      timeoutSeconds: 15
    ports:
    - containerPort: 10252
      hostport: 10252
    volumeMounts:
    - mountPath: /etc/kubernetes/controller
      name: etc-controller
      readOnly: false
  - name: kube-scheduler
    image: docker.io/whisperos/kube-scheduler:1.12.3
    command:
    - /kube-scheduler
    args:
    - --kubeconfig=/etc/kubernetes/scheduler/kubeconfig.yml
    livenessProbe:
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10251
        scheme: HTTP
      initialDelaySeconds: 15
      timeoutSeconds: 15
    ports:
    - containerPort: 10251
      hostPort: 10251
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler
      name: etc-scheduler
      readOnly: true
EOF"

sudo rc-service crio restart
sudo rc-service kubelet restart

kubectl config set-cluster ${DOMAIN} --server=https://${IP}:6443 --embed-certs=true --certificate-authority=ca_chain.pem
kubectl config set-credentials admin --embed-certs=true --client-certificate=admin.cert.pem --client-key=admin.key.pem
kubectl config set-context ${DOMAIN}-context --user admin --cluster ${DOMAIN}
kubectl config use-context ${DOMAIN}-context

while true ; do 
	kubectl version && break || sleep 1
done

while true ; do
	kubectl get clusterrole && break || sleep 1
done

cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
      - nodes/patch
    verbs:
      - "*"
EOF

cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: ${FQDN}
EOF
cat << EOF | kubectl apply -f -
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: calico-node
rules:
  - apiGroups: [""]
    resources:
      - namespaces
      - serviceaccounts
    verbs:
      - get
      - list
      - watch
  - apiGroups: [""]
    resources:
      - pods/status
    verbs:
      - patch
  - apiGroups: [""]
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
  - apiGroups: [""]
    resources:
      - services
    verbs:
      - get
  - apiGroups: [""]
    resources:
      - endpoints
    verbs:
      - get
  - apiGroups: [""]
    resources:
      - nodes
    verbs:
      - get
      - list
      - update
      - watch
  - apiGroups: ["extensions"]
    resources:
      - networkpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups: ["networking.k8s.io"]
    resources:
      - networkpolicies
    verbs:
      - watch
      - list
  - apiGroups: ["crd.projectcalico.org"]
    resources:
      - globalfelixconfigs
      - felixconfigurations
      - bgppeers
      - globalbgpconfigs
      - bgpconfigurations
      - ippools
      - globalnetworkpolicies
      - globalnetworksets
      - networkpolicies
      - clusterinformations
      - hostendpoints
    verbs:
      - create
      - get
      - list
      - update
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: calico-node
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: calico-node
subjects:
- kind: ServiceAccount
  name: calico-node
  namespace: kube-system
EOF
cat << EOF | kubectl apply -f -
kind: ConfigMap
apiVersion: v1
metadata:
  name: calico-config
  namespace: kube-system
data:
  typha_service_name: "none"
  calico_backend: "bird"
  veth_mtu: "1440"
  cni_network_config: |-
    {
      "name": "k8s-pod-network",
      "cniVersion": "0.3.0",
      "plugins": [
        {
          "type": "calico",
          "log_level": "info",
          "datastore_type": "kubernetes",
          "nodename": "__KUBERNETES_NODE_NAME__",
          "mtu": __CNI_MTU__,
          "ipam": {
            "type": "host-local",
            "subnet": "usePodCidr"
          },
          "policy": {
              "type": "k8s"
          },
          "kubernetes": {
              "kubeconfig": "__KUBECONFIG_FILEPATH__"
          }
        },
        {
          "type": "portmap",
          "snat": true,
          "capabilities": {"portMappings": true}
        }
      ]
    }
---
apiVersion: v1
kind: Service
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
  ports:
    - port: 5473
      protocol: TCP
      targetPort: calico-typha
      name: calico-typha
  selector:
    k8s-app: calico-typha

---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
  replicas: 0
  revisionHistoryLimit: 2
  template:
    metadata:
      labels:
        k8s-app: calico-typha
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'
    spec:
      nodeSelector:
        beta.kubernetes.io/os: linux
      hostNetwork: true
      tolerations:
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
      # Since Calico can't network a pod until Typha is up, we need to run Typha itself
      # as a host-networked pod.
      serviceAccountName: calico-node
      containers:
      - image: quay.io/calico/typha:v3.3.1
        name: calico-typha
        ports:
        - containerPort: 5473
          name: calico-typha
          protocol: TCP
        env:
          # Enable "info" logging by default.  Can be set to "debug" to increase verbosity.
          - name: TYPHA_LOGSEVERITYSCREEN
            value: "info"
          # Disable logging to file and syslog since those don't make sense in Kubernetes.
          - name: TYPHA_LOGFILEPATH
            value: "none"
          - name: TYPHA_LOGSEVERITYSYS
            value: "none"
          # Monitor the Kubernetes API to find the number of running instances and rebalance
          # connections.
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: "kubernetes"
          - name: TYPHA_DATASTORETYPE
            value: "kubernetes"
          - name: TYPHA_HEALTHENABLED
            value: "true"
          # Uncomment these lines to enable prometheus metrics.  Since Typha is host-networked,
          # this opens a port on the host, which may need to be secured.
          #- name: TYPHA_PROMETHEUSMETRICSENABLED
          #  value: "true"
          #- name: TYPHA_PROMETHEUSMETRICSPORT
          #  value: "9093"
        livenessProbe:
          exec:
            command:
            - calico-typha
            - check
            - liveness
          periodSeconds: 30
          initialDelaySeconds: 30
        readinessProbe:
          exec:
            command:
            - calico-typha
            - check
            - readiness
          periodSeconds: 10

---

# This manifest creates a Pod Disruption Budget for Typha to allow K8s Cluster Autoscaler to evict

apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: calico-typha

---

# This manifest installs the calico/node container, as well
# as the Calico CNI plugins and network config on
# each master and worker node in a Kubernetes cluster.
kind: DaemonSet
apiVersion: extensions/v1beta1
metadata:
  name: calico-node
  namespace: kube-system
  labels:
    k8s-app: calico-node
spec:
  selector:
    matchLabels:
      k8s-app: calico-node
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        k8s-app: calico-node
      annotations:
        # This, along with the CriticalAddonsOnly toleration below,
        # marks the pod as a critical add-on, ensuring it gets
        # priority scheduling and that its resources are reserved
        # if it ever gets evicted.
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      nodeSelector:
        beta.kubernetes.io/os: linux
      hostNetwork: true
      tolerations:
        # Make sure calico-node gets scheduled on all nodes.
        - effect: NoSchedule
          operator: Exists
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
      serviceAccountName: calico-node
      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force
      # deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
      terminationGracePeriodSeconds: 0
      containers:
        # Runs calico/node container on each Kubernetes node.  This
        # container programs network policy and routes on each
        # host.
        - name: calico-node
          image: quay.io/calico/node:v3.3.1
          env:
            # Use Kubernetes API as the backing datastore.
            - name: DATASTORE_TYPE
              value: "kubernetes"
            # Typha support: controlled by the ConfigMap.
            - name: FELIX_TYPHAK8SSERVICENAME
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: typha_service_name
            # Wait for the datastore.
            - name: WAIT_FOR_DATASTORE
              value: "true"
            # Set based on the k8s node name.
            - name: NODENAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # Choose the backend to use.
            - name: CALICO_NETWORKING_BACKEND
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: calico_backend
            # Cluster type to identify the deployment type
            - name: CLUSTER_TYPE
              value: "k8s,bgp"
            # Auto-detect the BGP IP address.
            - name: IP
              value: "autodetect"
            # Enable IPIP
            - name: CALICO_IPV4POOL_IPIP
              value: "Always"
            # Set MTU for tunnel device used if ipip is enabled
            - name: FELIX_IPINIPMTU
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: veth_mtu
            - name: CALICO_IPV4POOL_CIDR
              value: "${POD_CIDR}"
            - name: CALICO_DISABLE_FILE_LOGGING
              value: "true"
            # Set Felix endpoint to host default action to ACCEPT.
            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
              value: "ACCEPT"
            # Disable IPv6 on Kubernetes.
            - name: FELIX_IPV6SUPPORT
              value: "false"
            # Set Felix logging to "info"
            - name: FELIX_LOGSEVERITYSCREEN
              value: "info"
            - name: FELIX_HEALTHENABLED
              value: "true"
          securityContext:
            privileged: true
          resources:
            requests:
              cpu: 250m
          livenessProbe:
            httpGet:
              path: /liveness
              port: 9099
              host: localhost
            periodSeconds: 10
            initialDelaySeconds: 10
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
              - /bin/calico-node
              - -bird-ready
              - -felix-ready
            periodSeconds: 10
          volumeMounts:
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - mountPath: /run/xtables.lock
              name: xtables-lock
              readOnly: false
            - mountPath: /var/run/calico
              name: var-run-calico
              readOnly: false
            - mountPath: /var/lib/calico
              name: var-lib-calico
              readOnly: false
        # This container installs the Calico CNI binaries
        # and CNI network config file on each node.
        - name: install-cni
          image: quay.io/calico/cni:v3.3.1
          command: ["/install-cni.sh"]
          env:
            # Name of the CNI config file to create.
            - name: CNI_CONF_NAME
              value: "10-calico.conflist"
            # Set the hostname based on the k8s node name.
            - name: KUBERNETES_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # The CNI network config to install on each node.
            - name: CNI_NETWORK_CONFIG
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: cni_network_config
            # CNI MTU Config variable
            - name: CNI_MTU
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: veth_mtu
          volumeMounts:
            - mountPath: /host/opt/cni/bin
              name: cni-bin-dir
            - mountPath: /host/etc/cni/net.d
              name: cni-net-dir
      volumes:
        # Used by calico/node.
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: var-run-calico
          hostPath:
            path: /var/run/calico
        - name: var-lib-calico
          hostPath:
            path: /var/lib/calico
        - name: xtables-lock
          hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
        # Used to install CNI.
        - name: cni-bin-dir
          hostPath:
            path: /opt/cni/bin
        - name: cni-net-dir
          hostPath:
            path: /etc/cni/net.d
---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: calico-node
  namespace: kube-system

---

# Create all the CustomResourceDefinitions needed for
# Calico policy and networking mode.

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
   name: felixconfigurations.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: FelixConfiguration
    plural: felixconfigurations
    singular: felixconfiguration
---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: bgppeers.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: BGPPeer
    plural: bgppeers
    singular: bgppeer

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: bgpconfigurations.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: BGPConfiguration
    plural: bgpconfigurations
    singular: bgpconfiguration

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: ippools.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: IPPool
    plural: ippools
    singular: ippool

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: hostendpoints.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: HostEndpoint
    plural: hostendpoints
    singular: hostendpoint

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: clusterinformations.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: ClusterInformation
    plural: clusterinformations
    singular: clusterinformation

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: globalnetworkpolicies.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: GlobalNetworkPolicy
    plural: globalnetworkpolicies
    singular: globalnetworkpolicy

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: globalnetworksets.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: GlobalNetworkSet
    plural: globalnetworksets
    singular: globalnetworkset

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: networkpolicies.crd.projectcalico.org
spec:
  scope: Namespaced
  group: crd.projectcalico.org
  version: v1
  names:
    kind: NetworkPolicy
    plural: networkpolicies
    singular: networkpolicy
EOF

kubectl create -n kube-system secret tls kube-proxy-tls  --cert=system\:kube-proxy.cert.pem --key=system\:kube-proxy.key.pem
CA64=$(cat ca_chain.pem|base64 -w 0)
cat << EOF | kubectl apply -f -
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-proxy
  namespace: kube-system
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: system:kube-proxy
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
subjects:
  - kind: ServiceAccount
    name: kube-proxy
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:node-proxier
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-proxy-kube-config
  namespace: kube-system
data:
  kubeConfig: |-
    ---
    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority-data: ${CA64}
        server: https://${IP}:6443
      name: build-man
    contexts:
    - context:
        cluster: build-man
        user: kube-proxy
      name: build-man
    current-context: build-man
    kind: Config
    preferences: {}
    users:
    - name: kube-proxy
      user:
        client-certificate: /etc/kubernetes/certs/tls.crt
        client-key: /etc/kubernetes/certs/tls.key

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-proxy
  namespace: kube-system
data:
  proxyConfig: |-
    ---
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    bindAddress: "0.0.0.0"
    clientConnection:
      acceptContentTypes: ""
      burst: 10
      contentType: application/vnd.kubernetes.protobuf
      qps: 5
      kubeconfig: /etc/kubeconfig.yml
    clusterCIDR: "${POD_CIDR}"
    configSyncPeriod: 15m0s
    conntrack:
      max: 0
      maxPerCore: 32768
      min: 131072
      tcpCloseWaitTimeout: 1h0m0s
      tcpEstablishedTimeout: 24h0m0s
    enableProfiling: false
    healthzBindAddress: 0.0.0.0:10256
    iptables:
      masqueradeAll: false
      masqueradeBit: 14
      minSyncPeriod: 0s
      syncPeriod: 30s
    ipvs:
      excludeCIDRs: []
      minSyncPeriod: 0s
      scheduler: "rr"
      syncPeriod: 30s
    kind: KubeProxyConfiguration
    metricsBindAddress: 127.0.0.1:10249
    mode: "ipvs"
    nodePortAddresses: ["10.202.0.0/23"]
    oomScoreAdj: -999
    portRange: "21-30000"
    resourceContainer: /kube-proxy
    udpIdleTimeout: 250ms

---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  labels:
    k8s-app: kube-proxy
    addonmanager.kubernetes.io/mode: Reconcile
  name: kube-proxy
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: kube-proxy
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 10%
  template:
    metadata:
      labels:
        k8s-app: kube-proxy
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/os: linux
      tolerations:
      - operator: Exists
        effect: NoExecute
      - operator: Exists
        effect: NoSchedule
      - key: CriticalAddonsOnly
        operator: Exists
      containers:
      - name: kube-proxy
        image: docker.io/whisperos/kube-proxy:1.12.3
        command:
        - /kube-proxy
        args:
        - --config=/etc/kube-proxy.yml
        #resources:
        #  requests:
        #    cpu: {{ cpurequest }}
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /var/lib/iptables
          name: var-iptables
          readOnly: false
        - mountPath: /etc/kube-proxy.yml
          name: proxy-config
          subPath: kube-proxy.yml
        - mountPath: /etc/kubeconfig.yml
          name: kube-config
          subPath: kubeconfig.yml
        - mountPath: /run/xtables.lock
          name: xtables-lock
          readOnly: false
        - mountPath: /lib/modules
          name: lib-modules
          readOnly: true
        - mountPath: /etc/kubernetes/certs
          name: certs
          readOnly: true
      volumes:
      - name: proxy-config
        configMap:
          name: kube-proxy
          items:
          - key: proxyConfig
            path: kube-proxy.yml
      - name: kube-config
        configMap:
          name: kube-proxy-kube-config
          items:
          - key: kubeConfig
            path: kubeconfig.yml
      - name: certs
        secret:
          secretName: kube-proxy-tls
      - name: var-iptables
        hostPath:
          path: /var/lib/iptables
      - name: xtables-lock
        hostPath:
          path: /run/xtables.lock
          type: FileOrCreate
      - name: lib-modules
        hostPath:
          path: /lib/modules
      serviceAccountName: kube-proxy
EOF
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes ${DOMAIN} in-addr.arpa ip6.arpa {
          pods insecure
          upstream
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        proxy . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/name: "CoreDNS"
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: "kubernetes.io/hostname"
      serviceAccountName: coredns
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      containers:
      - name: coredns
        image: coredns/coredns:1.2.2
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        - name: etc-kubernetes
          readOnly: true
          mountPath: /etc/kubernetes
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
      dnsPolicy: Default
      volumes:
        - name: etc-kubernetes
          hostPath:
            path: /etc/kubernetes
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
---
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: "9153"
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: ${DNS_IP}
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
EOF
