#!/bin/bash

cat << EOF > ~/.cndtrc
org {
	o  Stark and Wayne
	ou StarkAndWayne
	l  Buffalo
	st NY
	c USA
}
EOF

FQDN=build-man.sw.cafe
POD_CIDR=10.202.32.0/19
SERVICE_CIDR=10.202.16.0/20

IP=$(ip addr show dev eth0 |grep inet |head -n1|awk '{print $2}'|sed -e 's/\/[0-9]\+$//')
HOST=$(echo $FQDN|sed -e 's/\..*$//')
DOMAIN=$(echo $FQDN|sed -e "s/$HOST\.//")
KUBE_IP=$(echo $SERVICE_CIDR|sed -e 's/\.[0-9]\+\/[0-9]\+$/\.1/')
DNS_IP=$(echo $SERVICE_CIDR|sed -e 's/\.[0-9]\+\/[0-9]\+$/\.53/')
KUBERNETES_VERSION=1.13.1


TMP=$(mktemp -d)
echo "Working dir: ${TMP}"
cd $TMP
git init .
mkdir -p certs
cd certs

conductor gen both $FQDN -i 127.0.0.1 -i $IP -i $KUBE_IP -d $HOST -d "kubernetes.default.svc.cluster.local"
conductor gen user system:kube-controller-manager -o system:kube-controller-manager
conductor gen user system:kube-scheduler -o system:kube-scheduler
conductor gen user system:kube-proxy -o system:node-proxier
conductor gen both system:node:$HOST -o system:nodes -d $FQDN -i $IP -d $HOST
conductor gen server service-account
conductor gen server      git.$DOMAIN
conductor gen server matchbox.$DOMAIN -d matchbox-rpc.$DOMAIN -d matchbox -d matchbox-rpc -i $IP
conductor gen user admin -o system:masters

git add *

sudo mkdir -p /etc/kubernetes/{manifests,certs,etcd,api,controller,scheduler}

function add_user(){
	getent passwd $1 > /dev/null \
		&& echo "user [$1] already exists, doing nothing" \
		|| sudo useradd -r $1
}

add_user etcd
add_user kube-scheduler
add_user kube-apiserver
add_user kube-controller-manager

sudo cp ca_chain.pem                      /etc/kubernetes/certs/ca.chain.pem
sudo cp system\:node\:$HOST.fullchain.pem /etc/kubernetes/certs/cert.fullchain.pem
sudo cp system\:node\:$HOST.key.pem       /etc/kubernetes/certs/cert.key.pem

sudo cp ca_chain.pem             /etc/kubernetes/api/ca.chain.pem
sudo cp $FQDN.fullchain.pem      /etc/kubernetes/api/cert.fullchain.pem
sudo cp $FQDN.key.pem            /etc/kubernetes/api/cert.key.pem
sudo cp service-account.cert.pem /etc/kubernetes/api/

sudo cp ca_chain.pem        /etc/kubernetes/etcd/ca.chain.pem
sudo cp $FQDN.fullchain.pem /etc/kubernetes/etcd/cert.fullchain.pem
sudo cp $FQDN.key.pem       /etc/kubernetes/etcd/cert.key.pem
sudo mkdir /var/lib/etcd

sudo cp ca_chain.pem                                  /etc/kubernetes/controller/ca.chain.pem
sudo cp system\:kube-controller-manager.key.pem       /etc/kubernetes/controller/cert.key.pem
sudo cp system\:kube-controller-manager.fullchain.pem /etc/kubernetes/controller/cert.fullchain.pem
sudo cp intermediate_cert.pem                         /etc/kubernetes/controller/intermediate.cert.pem
sudo cp intermediate_key.pem                          /etc/kubernetes/controller/intermediate.key.pem
sudo cp service-account.key.pem                       /etc/kubernetes/controller/

sudo cp ca_chain.pem                         /etc/kubernetes/scheduler/ca.chain.pem
sudo cp system\:kube-scheduler.key.pem       /etc/kubernetes/scheduler/cert.key.pem
sudo cp system\:kube-scheduler.fullchain.pem /etc/kubernetes/scheduler/cert.fullchain.pem

sudo chown root:root -R /etc/kubernetes 


ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)
GITEA_PASS=$(head -c 16 /dev/random | base64)
sudo su -c "cat << EOF > /etc/kubernetes/api/encryption.yml
---
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - secrets
    providers:
    - identity: {}
    - aesgcm:
        keys:
        - name: key1
          secret: ${ENCRYPTION_KEY}
EOF"
sudo su -c "cat << EOF > /etc/kubernetes/kubelet.yml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
evictionHard:
  memory.available:  \"200Mi\"
tlsCertFile:       /etc/kubernetes/certs/cert.fullchain.pem
tlsPrivateKeyFile: /etc/kubernetes/certs/cert.key.pem
staticPodPath:     /etc/kubernetes/manifests
address: ${IP}
CgroupDriver:      cgroupfs
CgroupRoot:        /
clusterDomain: cluster.local
clusterDNS:
  - ${DNS_IP}
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/certs/ca.chain.pem
authorization:
  mode: Webhook
KubeletAuthentication:
  KubeletX509Authentication:
    ClientCAFile: /etc/kubernetes/certs/ca.chain.pem
EOF"

sudo su -c "cat << EOF > /etc/kubernetes/kubeconfig.yml
apiVersion: v1
kind: Config
clusters:
- name: sw
  cluster:
    server: https://${IP}:6443
    certificate-authority: /etc/kubernetes/certs/ca.chain.pem
users:
- name: kubelet
  user:
    client-certificate: /etc/kubernetes/certs/cert.fullchain.pem
    client-key:         /etc/kubernetes/certs/cert.key.pem
contexts:
- context:
    cluster: sw
    user: kubelet
  name: sw-context
current-context: sw-context
EOF"
sudo su -c "cat << EOF > /etc/kubernetes/controller/kubeconfig.yml
apiVersion: v1
kind: Config
clusters:
- name: sw
  cluster:
    server: https://${IP}:6443
    certificate-authority: /etc/kubernetes/controller/ca.chain.pem
users:
- name: kubelet
  user:
    client-certificate: /etc/kubernetes/controller/cert.fullchain.pem
    client-key:         /etc/kubernetes/controller/cert.key.pem
contexts:
- context:
    cluster: sw
    user: kubelet
  name: sw-context
current-context: sw-context
EOF"
sudo su -c "cat << EOF > /etc/kubernetes/scheduler/kubeconfig.yml
apiVersion: v1
kind: Config
clusters:
- name: sw
  cluster:
    server: https://${IP}:6443
    certificate-authority: /etc/kubernetes/scheduler/ca.chain.pem
users:
- name: kubelet
  user:
    client-certificate: /etc/kubernetes/scheduler/cert.fullchain.pem
    client-key:         /etc/kubernetes/scheduler/cert.key.pem
contexts:
- context:
    cluster: sw
    user: kubelet
  name: sw-context
current-context: sw-context
EOF"

sudo chown -R kube-controller-manager:kube-controller-manager /etc/kubernetes/controller
sudo chown -R etcd:etcd /etc/kubernetes/etcd
sudo chown -R etcd:etcd /var/lib/etcd
sudo chown -R kube-scheduler:kube-scheduler /etc/kubernetes/scheduler
sudo chown -R kube-apiserver:kube-apiserver /etc/kubernetes/api

ETCD_UID=$(id etcd -u)
ETCD_GID=$(id etcd -g)
CONTROLLER_UID=$(id kube-controller-manager -u)
CONTROLLER_GID=$(id kube-controller-manager -g)
API_UID=$(id kube-apiserver -u)
API_GID=$(id kube-apiserver -g)
SCHEDULER_UID=$(id kube-scheduler -u)
SCHEDULER_GID=$(id kube-scheduler -g)

sudo su -c "cat << EOF > /etc/kubernetes/manifests/etcd.yml
---
apiVersion: v1
kind: Pod
metadata:
  name: etcd
  namespace: kube-system
spec:
  securityContext:
    runAsUser: ${ETCD_UID}
    fsGroup: ${ETCD_GID}
  containers:
  - image: docker.io/whisperos/etcd:3.3.10
    command:
    - /etcd
    args:
    - --name=${FQDN}
    - --data-dir=/var/lib/etcd
    - --trusted-ca-file=/etc/kubernetes/etcd/ca.chain.pem
    - --peer-trusted-ca-file=/etc/kubernetes/etcd/ca.chain.pem
    - --client-cert-auth
    - --cert-file=/etc/kubernetes/etcd/cert.fullchain.pem
    - --key-file=/etc/kubernetes/etcd/cert.key.pem
    - --peer-cert-file=/etc/kubernetes/etcd/cert.fullchain.pem
    - --peer-key-file=/etc/kubernetes/etcd/cert.key.pem
    - --enable-v2=false
    - --peer-auto-tls=false
    - --initial-cluster-state=new
    - --initial-cluster-token='boot-kube-etcd-cluster'
    - --initial-cluster
    - '${FQDN}=https://${IP}:2380'
    - --listen-peer-urls
    - https://${IP}:2380
    - --listen-client-urls
    - http://127.0.0.1:2379,https://${IP}:2379
    - --initial-advertise-peer-urls
    - https://${FQDN}:2380
    - --advertise-client-urls
    - http://127.0.0.1:2379,https://${IP}:2379,https://${FQDN}:2379
    #livenessProbe:
    #  httpGet:
    #    host: 127.0.0.1
    #    path: /health
    #    port: 2379
    #    schema: HTTP
    #  initialDelaySeconds: 5
    #  timeoutSeconds: 15
    name: etcd
    ports:
    - containerPort: 2380
      hostPort: 2380
      name: serverport
    - containerPort: 2379
      hostPort: 2379
      name: clientport
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: lib
      readOnly: false
    - mountPath: /var/log/etcd.log
      name: log
      readOnly: false
    - mountPath: /etc/kubernetes/etcd
      name: etc
      readOnly: true
  hostNetwork: true
  volumes:
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: lib
  - hostPath:
      path: /var/log/etcd.log
      type: FileOrCreate
    name: log
  - hostPath:
      path: /etc/kubernetes/etcd
    name: etc
EOF"
sudo su -c "cat << EOF > /etc/kubernetes/manifests/kube-apiserver.yml
---
apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  securityContext:
    runAsUser: ${API_UID}
    fsGroup: ${API_GID}
  hostNetwork: true
  volumes:
  - hostPath:
      path: /etc/kubernetes/api
    name: etc-api
  containers:
  - image: docker.io/whisperos/kube-apiserver:${KUBERNETES_VERSION}
    name: kube-apiserver
    command:
    - /kube-apiserver
    args:
    - --etcd-servers=https://${IP}:2379
    - --etcd-cafile=/etc/kubernetes/api/ca.chain.pem
    - --etcd-certfile=/etc/kubernetes/api/cert.fullchain.pem
    - --etcd-keyfile=/etc/kubernetes/api/cert.key.pem
    - --tls-cert-file=/etc/kubernetes/api/cert.fullchain.pem
    - --tls-private-key-file=/etc/kubernetes/api/cert.key.pem
    - --kubelet-certificate-authority=/etc/kubernetes/api/ca.chain.pem
    - --kubelet-client-certificate=/etc/kubernetes/api/cert.fullchain.pem
    - --kubelet-client-key=/etc/kubernetes/api/cert.key.pem
    - --cert-dir=/etc/kubernetes/api
    - --cloud-provider=external
    - --allow-privileged
    - --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota
    - --client-ca-file=/etc/kubernetes/api/ca.chain.pem
    - --encryption-provider-config=/etc/kubernetes/api/encryption.yml
    - --external-hostname=${FQDN}
    - --insecure-port=0
    - --service-cluster-ip-range=${SERVICE_CIDR}
    - --service-node-port-range=21-30000
    - --bind-address=${IP}
    - --advertise-address=${IP}
    - --apiserver-count=1
    - --endpoint-reconciler-type=master-count
    - --authorization-mode=Node,RBAC
    - --enable-aggregator-routing=true
    - --service-account-key-file=/etc/kubernetes/api/service-account.cert.pem
    - --requestheader-client-ca-file=/etc/kubernetes/api/ca.chain.pem
    - --requestheader-allowed-names=aggregator,metrics-server,admin,system:kube-proxy,system:kube-controller-manager,system:kube-scheduler
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    livenessProbe:
      httpGet:
        host: \"${IP}\"
        path: /healthz
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 15
      timeoutSeconds: 15
    ports:
    - containerPort: 6443
      hostPort: 6443
      name: https
    volumeMounts:
    - mountPath: /etc/kubernetes/api
      name: etc-api
      readOnly: true
EOF"
sudo su -c "cat << EOF > /etc/kubernetes/manifests/kube-controller-manager.yml
---
apiVersion: v1
kind: Pod
metadata:
  name: kube-controller-manager
  namespace: kube-system
spec:
  securityContext:
    runAsUser: ${CONTROLLER_UID}
    fsGroup: ${CONTROLLER_GID}
  hostNetwork: true
  volumes:
  - hostPath:
      path: /etc/kubernetes/controller
    name: etc-controller
  containers:
  - name: kube-controller-manager
    image: docker.io/whisperos/kube-controller-manager:${KUBERNETES_VERSION}
    command:
    - /kube-controller-manager
    args:
    - --kubeconfig=/etc/kubernetes/controller/kubeconfig.yml
    - --cluster-name=sw
    - --tls-cert-file=/etc/kubernetes/controller/cert.fullchain.pem
    - --tls-private-key-file=/etc/kubernetes/controller/cert.key.pem
    - --cluster-signing-cert-file=/etc/kubernetes/controller/intermediate.cert.pem
    - --cluster-signing-key-file=/etc/kubernetes/controller/intermediate.key.pem
    - --root-ca-file=/etc/kubernetes/controller/ca.chain.pem
    - --service-account-private-key-file=/etc/kubernetes/controller/service-account.key.pem
    - --cloud-provider=external
    - --allocate-node-cidrs
    - --cluster-cidr=${POD_CIDR}
    - --service-cluster-ip-range=${SERVICE_CIDR}
    - --use-service-account-credentials=true
    livenessProbe:
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10252
        scheme: HTTP
      initialDelaySeconds: 15
      timeoutSeconds: 15
    ports:
    - containerPort: 10252
      hostport: 10252
    volumeMounts:
    - mountPath: /etc/kubernetes/controller
      name: etc-controller
      readOnly: false
EOF"
sudo su -c "cat << EOF > /etc/kubernetes/manifests/kube-scheduler.yml
---
apiVersion: v1
kind: Pod
metadata:
  name: kube-scheduler
  namespace: kube-system
spec:
  securityContext:
    runAsUser: ${SCHEDULER_UID}
    fsGroup: ${SCHEDULER_GID}
  hostNetwork: true
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler
    name: etc-scheduler
  containers:
  - name: kube-scheduler
    image: docker.io/whisperos/kube-scheduler:${KUBERNETES_VERSION}
    command:
    - /kube-scheduler
    args:
    - --kubeconfig=/etc/kubernetes/scheduler/kubeconfig.yml
    livenessProbe:
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10251
        scheme: HTTP
      initialDelaySeconds: 15
      timeoutSeconds: 15
    ports:
    - containerPort: 10251
      hostPort: 10251
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler
      name: etc-scheduler
      readOnly: true
EOF"

sudo rc-service crio restart
sudo rc-service kubelet restart

kubectl config set-cluster ${DOMAIN} --server=https://${IP}:6443 --embed-certs=true --certificate-authority=ca_chain.pem
kubectl config set-credentials admin --embed-certs=true --client-certificate=admin.cert.pem --client-key=admin.key.pem
kubectl config set-context ${DOMAIN}-context --user admin --cluster ${DOMAIN}
kubectl config use-context ${DOMAIN}-context

while true ; do 
	kubectl version && break || sleep 1
done

while true ; do
	kubectl get clusterrole && break || sleep 1
done

mkdir ${TMP}/kube-system
cat <<EOF > ${TMP}/kube-system/rbac.yml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
      - nodes/patch
    verbs:
      - "*"
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: ${FQDN}
EOF
kubectl apply -f ${TMP}/kube-system/rbac.yml
git add ${TMP}/kube-system/rbac.yml

mkdir ${TMP}/calico
cat << EOF > ${TMP}/calico/calico.yml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: calico-node
rules:
  - apiGroups: [""]
    resources:
      - namespaces
      - serviceaccounts
    verbs:
      - get
      - list
      - watch
  - apiGroups: [""]
    resources:
      - pods/status
    verbs:
      - patch
  - apiGroups: [""]
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
  - apiGroups: [""]
    resources:
      - services
    verbs:
      - get
  - apiGroups: [""]
    resources:
      - endpoints
    verbs:
      - get
  - apiGroups: [""]
    resources:
      - nodes
    verbs:
      - get
      - list
      - update
      - watch
  - apiGroups: ["extensions"]
    resources:
      - networkpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups: ["networking.k8s.io"]
    resources:
      - networkpolicies
    verbs:
      - watch
      - list
  - apiGroups: ["crd.projectcalico.org"]
    resources:
      - globalfelixconfigs
      - felixconfigurations
      - bgppeers
      - globalbgpconfigs
      - bgpconfigurations
      - ippools
      - globalnetworkpolicies
      - globalnetworksets
      - networkpolicies
      - clusterinformations
      - hostendpoints
    verbs:
      - create
      - get
      - list
      - update
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: calico-node
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: calico-node
subjects:
- kind: ServiceAccount
  name: calico-node
  namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: calico-config
  namespace: kube-system
data:
  typha_service_name: "none"
  calico_backend: "bird"
  veth_mtu: "1440"
  cni_network_config: |-
    {
      "name": "k8s-pod-network",
      "cniVersion": "0.3.0",
      "plugins": [
        {
          "type": "calico",
          "log_level": "info",
          "datastore_type": "kubernetes",
          "nodename": "__KUBERNETES_NODE_NAME__",
          "mtu": __CNI_MTU__,
          "ipam": {
            "type": "host-local",
            "subnet": "usePodCidr"
          },
          "policy": {
              "type": "k8s"
          },
          "kubernetes": {
              "kubeconfig": "__KUBECONFIG_FILEPATH__"
          }
        },
        {
          "type": "portmap",
          "snat": true,
          "capabilities": {"portMappings": true}
        }
      ]
    }
---
apiVersion: v1
kind: Service
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
  ports:
    - port: 5473
      protocol: TCP
      targetPort: calico-typha
      name: calico-typha
  selector:
    k8s-app: calico-typha

---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
  replicas: 0
  revisionHistoryLimit: 2
  template:
    metadata:
      labels:
        k8s-app: calico-typha
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'
    spec:
      nodeSelector:
        beta.kubernetes.io/os: linux
      hostNetwork: true
      tolerations:
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
      # Since Calico can't network a pod until Typha is up, we need to run Typha itself
      # as a host-networked pod.
      serviceAccountName: calico-node
      containers:
      - image: quay.io/calico/typha:v3.3.1
        name: calico-typha
        ports:
        - containerPort: 5473
          name: calico-typha
          protocol: TCP
        env:
          # Enable "info" logging by default.  Can be set to "debug" to increase verbosity.
          - name: TYPHA_LOGSEVERITYSCREEN
            value: "info"
          # Disable logging to file and syslog since those don't make sense in Kubernetes.
          - name: TYPHA_LOGFILEPATH
            value: "none"
          - name: TYPHA_LOGSEVERITYSYS
            value: "none"
          # Monitor the Kubernetes API to find the number of running instances and rebalance
          # connections.
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: "kubernetes"
          - name: TYPHA_DATASTORETYPE
            value: "kubernetes"
          - name: TYPHA_HEALTHENABLED
            value: "true"
          # Uncomment these lines to enable prometheus metrics.  Since Typha is host-networked,
          # this opens a port on the host, which may need to be secured.
          #- name: TYPHA_PROMETHEUSMETRICSENABLED
          #  value: "true"
          #- name: TYPHA_PROMETHEUSMETRICSPORT
          #  value: "9093"
        livenessProbe:
          exec:
            command:
            - calico-typha
            - check
            - liveness
          periodSeconds: 30
          initialDelaySeconds: 30
        readinessProbe:
          exec:
            command:
            - calico-typha
            - check
            - readiness
          periodSeconds: 10

---

# This manifest creates a Pod Disruption Budget for Typha to allow K8s Cluster Autoscaler to evict

apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: calico-typha

---

# This manifest installs the calico/node container, as well
# as the Calico CNI plugins and network config on
# each master and worker node in a Kubernetes cluster.
kind: DaemonSet
apiVersion: extensions/v1beta1
metadata:
  name: calico-node
  namespace: kube-system
  labels:
    k8s-app: calico-node
spec:
  selector:
    matchLabels:
      k8s-app: calico-node
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        k8s-app: calico-node
      annotations:
        # This, along with the CriticalAddonsOnly toleration below,
        # marks the pod as a critical add-on, ensuring it gets
        # priority scheduling and that its resources are reserved
        # if it ever gets evicted.
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      nodeSelector:
        beta.kubernetes.io/os: linux
      hostNetwork: true
      tolerations:
        # Make sure calico-node gets scheduled on all nodes.
        - effect: NoSchedule
          operator: Exists
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
      serviceAccountName: calico-node
      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force
      # deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
      terminationGracePeriodSeconds: 0
      containers:
        # Runs calico/node container on each Kubernetes node.  This
        # container programs network policy and routes on each
        # host.
        - name: calico-node
          image: quay.io/calico/node:v3.3.1
          env:
            # Use Kubernetes API as the backing datastore.
            - name: DATASTORE_TYPE
              value: "kubernetes"
            # Typha support: controlled by the ConfigMap.
            - name: FELIX_TYPHAK8SSERVICENAME
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: typha_service_name
            # Wait for the datastore.
            - name: WAIT_FOR_DATASTORE
              value: "true"
            # Set based on the k8s node name.
            - name: NODENAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # Choose the backend to use.
            - name: CALICO_NETWORKING_BACKEND
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: calico_backend
            # Cluster type to identify the deployment type
            - name: CLUSTER_TYPE
              value: "k8s,bgp"
            # Auto-detect the BGP IP address.
            - name: IP
              value: "autodetect"
            # Enable IPIP
            - name: CALICO_IPV4POOL_IPIP
              value: "Always"
            # Set MTU for tunnel device used if ipip is enabled
            - name: FELIX_IPINIPMTU
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: veth_mtu
            - name: CALICO_IPV4POOL_CIDR
              value: "${POD_CIDR}"
            - name: CALICO_DISABLE_FILE_LOGGING
              value: "true"
            # Set Felix endpoint to host default action to ACCEPT.
            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
              value: "ACCEPT"
            # Disable IPv6 on Kubernetes.
            - name: FELIX_IPV6SUPPORT
              value: "false"
            # Set Felix logging to "info"
            - name: FELIX_LOGSEVERITYSCREEN
              value: "info"
            - name: FELIX_HEALTHENABLED
              value: "true"
          securityContext:
            privileged: true
          resources:
            requests:
              cpu: 250m
          livenessProbe:
            httpGet:
              path: /liveness
              port: 9099
              host: localhost
            periodSeconds: 10
            initialDelaySeconds: 10
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
              - /bin/calico-node
              - -bird-ready
              - -felix-ready
            periodSeconds: 10
          volumeMounts:
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - mountPath: /run/xtables.lock
              name: xtables-lock
              readOnly: false
            - mountPath: /var/run/calico
              name: var-run-calico
              readOnly: false
            - mountPath: /var/lib/calico
              name: var-lib-calico
              readOnly: false
        # This container installs the Calico CNI binaries
        # and CNI network config file on each node.
        - name: install-cni
          image: quay.io/calico/cni:v3.3.1
          command: ["/install-cni.sh"]
          env:
            # Name of the CNI config file to create.
            - name: CNI_CONF_NAME
              value: "10-calico.conflist"
            # Set the hostname based on the k8s node name.
            - name: KUBERNETES_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # The CNI network config to install on each node.
            - name: CNI_NETWORK_CONFIG
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: cni_network_config
            # CNI MTU Config variable
            - name: CNI_MTU
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: veth_mtu
          volumeMounts:
            - mountPath: /host/opt/cni/bin
              name: cni-bin-dir
            - mountPath: /host/etc/cni/net.d
              name: cni-net-dir
      volumes:
        # Used by calico/node.
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: var-run-calico
          hostPath:
            path: /var/run/calico
        - name: var-lib-calico
          hostPath:
            path: /var/lib/calico
        - name: xtables-lock
          hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
        # Used to install CNI.
        - name: cni-bin-dir
          hostPath:
            path: /opt/cni/bin
        - name: cni-net-dir
          hostPath:
            path: /etc/cni/net.d
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: calico-node
  namespace: kube-system
---
# Create all the CustomResourceDefinitions needed for
# Calico policy and networking mode.
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
   name: felixconfigurations.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: FelixConfiguration
    plural: felixconfigurations
    singular: felixconfiguration
---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: bgppeers.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: BGPPeer
    plural: bgppeers
    singular: bgppeer

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: bgpconfigurations.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: BGPConfiguration
    plural: bgpconfigurations
    singular: bgpconfiguration

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: ippools.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: IPPool
    plural: ippools
    singular: ippool

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: hostendpoints.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: HostEndpoint
    plural: hostendpoints
    singular: hostendpoint

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: clusterinformations.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: ClusterInformation
    plural: clusterinformations
    singular: clusterinformation

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: globalnetworkpolicies.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: GlobalNetworkPolicy
    plural: globalnetworkpolicies
    singular: globalnetworkpolicy

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: globalnetworksets.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: GlobalNetworkSet
    plural: globalnetworksets
    singular: globalnetworkset

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: networkpolicies.crd.projectcalico.org
spec:
  scope: Namespaced
  group: crd.projectcalico.org
  version: v1
  names:
    kind: NetworkPolicy
    plural: networkpolicies
    singular: networkpolicy
EOF
kubectl apply -f ${TMP}/calico/calico.yml
git add ${TMP}/calico/calico.yml

kubectl create -n kube-system secret tls kube-proxy-tls  --cert=system\:kube-proxy.cert.pem --key=system\:kube-proxy.key.pem
CA64=$(cat ca_chain.pem|base64 -w 0)
cat << EOF > ${TMP}/kube-system/kube-proxy.yml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-proxy
  namespace: kube-system
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: system:kube-proxy
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
subjects:
  - kind: ServiceAccount
    name: kube-proxy
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:node-proxier
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-proxy-kube-config
  namespace: kube-system
data:
  kubeConfig: |-
    ---
    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority-data: ${CA64}
        server: https://${IP}:6443
      name: build-man
    contexts:
    - context:
        cluster: build-man
        user: kube-proxy
      name: build-man
    current-context: build-man
    kind: Config
    preferences: {}
    users:
    - name: kube-proxy
      user:
        client-certificate: /etc/kubernetes/certs/tls.crt
        client-key: /etc/kubernetes/certs/tls.key
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-proxy
  namespace: kube-system
data:
  proxyConfig: |-
    ---
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    bindAddress: "0.0.0.0"
    clientConnection:
      acceptContentTypes: ""
      burst: 10
      contentType: application/vnd.kubernetes.protobuf
      qps: 5
      kubeconfig: /etc/kubeconfig.yml
    clusterCIDR: "${POD_CIDR}"
    configSyncPeriod: 15m0s
    conntrack:
      max: 0
      maxPerCore: 32768
      min: 131072
      tcpCloseWaitTimeout: 1h0m0s
      tcpEstablishedTimeout: 24h0m0s
    enableProfiling: false
    healthzBindAddress: 0.0.0.0:10256
    iptables:
      masqueradeAll: false
      masqueradeBit: 14
      minSyncPeriod: 0s
      syncPeriod: 30s
    ipvs:
      excludeCIDRs: []
      minSyncPeriod: 0s
      scheduler: "rr"
      syncPeriod: 30s
    kind: KubeProxyConfiguration
    metricsBindAddress: 127.0.0.1:10249
    mode: "ipvs"
    nodePortAddresses: ["10.202.0.0/23"]
    oomScoreAdj: -999
    portRange: "21-30000"
    resourceContainer: /kube-proxy
    udpIdleTimeout: 250ms

---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  labels:
    k8s-app: kube-proxy
    addonmanager.kubernetes.io/mode: Reconcile
  name: kube-proxy
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: kube-proxy
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 10%
  template:
    metadata:
      labels:
        k8s-app: kube-proxy
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/os: linux
      tolerations:
      - operator: Exists
        effect: NoExecute
      - operator: Exists
        effect: NoSchedule
      - key: CriticalAddonsOnly
        operator: Exists
      containers:
      - name: kube-proxy
        image: docker.io/whisperos/kube-proxy:${KUBERNETES_VERSION}
        command:
        - /kube-proxy
        args:
        - --config=/etc/kube-proxy.yml
        #resources:
        #  requests:
        #    cpu: {{ cpurequest }}
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /var/lib/iptables
          name: var-iptables
          readOnly: false
        - mountPath: /etc/kube-proxy.yml
          name: proxy-config
          subPath: kube-proxy.yml
        - mountPath: /etc/kubeconfig.yml
          name: kube-config
          subPath: kubeconfig.yml
        - mountPath: /run/xtables.lock
          name: xtables-lock
          readOnly: false
        - mountPath: /lib/modules
          name: lib-modules
          readOnly: true
        - mountPath: /etc/kubernetes/certs
          name: certs
          readOnly: true
      volumes:
      - name: proxy-config
        configMap:
          name: kube-proxy
          items:
          - key: proxyConfig
            path: kube-proxy.yml
      - name: kube-config
        configMap:
          name: kube-proxy-kube-config
          items:
          - key: kubeConfig
            path: kubeconfig.yml
      - name: certs
        secret:
          secretName: kube-proxy-tls
      - name: var-iptables
        hostPath:
          path: /var/lib/iptables
      - name: xtables-lock
        hostPath:
          path: /run/xtables.lock
          type: FileOrCreate
      - name: lib-modules
        hostPath:
          path: /lib/modules
      serviceAccountName: kube-proxy
EOF
kubectl apply -f ${TMP}/kube-system/kube-proxy.yml
git add ${TMP}/kube-system/kube-proxy.yml

mkdir ${TMP}/dns
cat << EOF > ${TMP}/dns/coredns.yml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          upstream
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        proxy . /etc/resolv.conf
        file /etc/coredns/${DOMAIN}.db ${DOMAIN}
        cache 30
        loop
        reload
        loadbalance
    }
  ${DOMAIN}.db: |
    ; Testing a static Zone
    ${DOMAIN}.              IN SOA ${DOMAIN}. ${FQDN}. $(date +%Y%m%d)01 7200 3600 1209600 3600
    ${DOMAIN}.              IN NS  ns1.${DOMAIN}.
    ns1.${DOMAIN}           IN A   ${IP}
    ${DOMAIN}.              IN A   ${IP}
    ${FQDN}.                IN A   ${IP}
    matchbox.${DOMAIN}.     IN A   ${IP}
    matchbox-rpc.${DOMAIN}. IN A   ${IP}
    git.${DOMAIN}.          IN A   ${IP}
    drone.${DOMAIN}.        IN A   ${IP}
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/name: "CoreDNS"
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: "kubernetes.io/hostname"
      serviceAccountName: coredns
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      containers:
      - name: coredns
        image: coredns/coredns:1.2.2
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
            - key: ${DOMAIN}.db
              path: ${DOMAIN}.db
---
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: "9153"
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: ${DNS_IP}
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: kube-dns
  name: kube-dns-ingress
  namespace: kube-system
spec:
  ports:
  - name: dns
    port: 53
    protocol: UDP
    targetPort: 53
    nodePort: 53
  - name: dns-tcp
    port: 53
    protocol: TCP
    targetPort: 53
    nodePort: 53
  selector:
    k8s-app: kube-dns
  type: NodePort
EOF
kubectl apply -f ${TMP}/dns/coredns.yml
git add ${TMP}/dns/coredns.yml

mkdir ${TMP}/ingress
cat << EOF > ${TMP}/ingress/nginx.yml
---
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses/status
    verbs:
      - update
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
    spec:
      serviceAccountName: nginx-ingress-serviceaccount
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
          args:
            - /nginx-ingress-controller
            - --configmap=\$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=\$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=\$(POD_NAMESPACE)/udp-services
            - --publish-service=\$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            # www-data -> 33
            runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
---
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 80
      targetPort: 80
      nodePort: 80
      protocol: TCP
    - name: https
      port: 443
      nodePort: 443
      targetPort: 443
      protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
EOF
kubectl apply -f ${TMP}/ingress/nginx.yml
git add ${TMP}/ingress/nginx.yml

mkdir ${TMP}/ci
cat << EOF > ${TMP}/ci/ns.yml
---
kind: Namespace
apiVersion: v1
metadata:
  name: ci
  labels:
    name: ci
EOF
kubectl apply -f ${TMP}/ci/ns.yml
git add ${TMP}/ci/ns.yml



kubectl create -n ci secret tls git-tls --cert git.${DOMAIN}.fullchain.pem --key git.${DOMAIN}.key.pem
cat << EOF > ${TMP}/ci/gitea.yml
---
apiVersion: v1
kind: Service
metadata:
  name: pg-git
  namespace: ci
  labels:
    app: postgres
    service: database
spec:
  ports:
  - name: postgres
    protocol: TCP
    port: 5432
    targetPort: 5432
  selector:
    app: postgres
    service: database
  type: ClusterIP
---
apiVersion: v1
kind: Secret
metadata:
  name: pg-git-secrets
  namespace: ci
type: Opaque
stringData:
  db_name: gitea
  db_user: gitea
  db_pass: AVerySecretDataBasePassword
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: pg-git
  namespace: ci
  labels:
    app: postgres
    service: database
spec:
  serviceName: pg-git
  replicas: 1
  template:
    metadata:
      labels:
        app: postgres
        service: database
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: postgres
        image: postgres:alpine
        ports:
        - name: postgres
          containerPort: 5432
        env:
        - name: PGDATA
          value: /var/lib/postgresql/data
        - name: POSTGRES_DB
          valueFrom:
            secretKeyRef:
              name: pg-git-secrets
              key: db_name
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: pg-git-secrets
              key: db_user
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: pg-git-secrets
              key: db_pass
        resources:
          requests:
            cpu: "0.01"
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data
      restartPolicy: Always
      volumes:
      - name: postgres-data
        hostPath:
          path: /var/lib/postgresql/git
          type: DirectoryOrCreate
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
  namespace: ci
  labels:
    app: redis
    role: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
  template:
    metadata:
      labels:
        app: redis
        role: master
    spec:
      containers:
      - name: redis-master
        image: redis:alpine
        ports:
        - containerPort: 6379
          protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  name: redis-master
  namespace: ci
  labels:
    app:  redis
    role: master
spec:
  ports:
  - port: 6379
    protocol: TCP
    name: "redis-server"
  selector:
    app:  redis
    role: master
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: git-config
  namespace: ci
data:
  app.ini: |-
    APP_NAME = Stark & Wayne :: Gitea
    RUN_MODE = prod
    RUN_USER = git
    
    [repository]
    ROOT = /data/git/repositories
    
    [repository.local]
    LOCAL_COPY_PATH = /data/gitea/tmp/local-repo
    
    [repository.upload]
    TEMP_PATH = /data/gitea/uploads
    
    [server]
    APP_DATA_PATH    = /data/gitea
    SSH_DOMAIN       = git.$DOMAIN
    HTTP_PORT        = 3000
    ROOT_URL         = https://git.$DOMAIN/
    DISABLE_SSH      = false
    SSH_PORT         = 22
    LFS_CONTENT_PATH = /data/git/lfs
    DOMAIN           = git.$DOMAIN
    LFS_START_SERVER = true
    LFS_JWT_SECRET   = Xz27yLum1XIlPLaOB28L3PcYa0mEEKX9wdTJqBHIV_Q
    OFFLINE_MODE     = true
    
    [database]
    DB_TYPE  = postgres
    HOST     = pg-git.ci.svc.cluster.local:5432
    NAME     = gitea
    USER     = gitea
    PASSWD   = AVerySecretDataBasePassword
    SSL_MODE = disable
    
    [indexer]
    ISSUE_INDEXER_PATH = /data/gitea/indexers/issues.bleve
    
    [session]
    PROVIDER_CONFIG = network=tcp,addr=redis-master.ci.svc.cluster.local:6379,db=0,pool_size=100,idle_timeout=180
    PROVIDER        = redis
    
    [cache]
    ADAPTER = redis
    HOST = network=tcp,addr=redis-master.ci.svc.cluster.local:6379,db=1,pool_size=100,idle_timeout=180
    
    [picture]
    AVATAR_UPLOAD_PATH      = /data/gitea/avatars
    DISABLE_GRAVATAR        = true
    ENABLE_FEDERATED_AVATAR = false
    
    [attachment]
    PATH = /data/gitea/attachments
    
    [log]
    ROOT_PATH = /data/gitea/log
    MODE      = file
    LEVEL     = Info
    
    [security]
    INSTALL_LOCK   = true
    SECRET_KEY     = VOdBXJR5XFVve5NjRPtHZl33ZUm4ym6QnxivSEYQKx6CBnYGRlPKr8iZj2l2zQzb
    INTERNAL_TOKEN = eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYmYiOjE1NDM2MTY3MDd9.t-15zblEPiiEncnnzzD-MbVrXjnk5537vhtxZ8Z9Eg0
    
    [service]
    DISABLE_REGISTRATION              = false
    REQUIRE_SIGNIN_VIEW               = true
    REGISTER_EMAIL_CONFIRM            = false
    ENABLE_NOTIFY_MAIL                = false
    ALLOW_ONLY_EXTERNAL_REGISTRATION  = false
    ENABLE_CAPTCHA                    = false
    DEFAULT_KEEP_EMAIL_PRIVATE        = false
    DEFAULT_ALLOW_CREATE_ORGANIZATION = true
    DEFAULT_ENABLE_TIMETRACKING       = true
    NO_REPLY_ADDRESS                  = noreply.example.org
    
    [mailer]
    ENABLED = false
    
    [openid]
    ENABLE_OPENID_SIGNIN = true
    ENABLE_OPENID_SIGNUP = true

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gitea
  namespace: ci
  labels:
    app: gitea
spec:
  replicas: 1
  selector:
      matchLabels:
        app: gitea
  template:
    metadata:
      labels:
        app: gitea
    spec:
      volumes:
      - name: git
        hostPath:
          path: /var/lib/git
          type: DirectoryOrCreate
      - name: config
        configMap:
          name: git-config
          items:
          - key: app.ini
            path: app.ini
      containers:
      - name: init-gitea
        image: "gitea/gitea:1.6.0"
        command: ['sh', '-c', 'until gitea admin create-user --name coreseed --password ${GITEA_PASS} --email coressed@${DOMAIN} --admin; do echo "waiting to create admin user" ; sleep 3; done; while true ; do sleep 3600 ; done']
        volumeMounts:
        - name: git
          mountPath: "/data"
        - name: config
          mountPath: /data/gitea/conf/app.ini
          subPath: app.ini
      - name: gitea
        image: "gitea/gitea:1.6.0"
        livenessProbe:
          httpGet:
            port: http
          initialDelaySeconds: 30
          periodSeconds: 5
        readinessProbe:
          httpGet:
            port: http
          initialDelaySeconds: 15
          periodSeconds: 5
        ports:
        - containerPort: 3000
          protocol: TCP
          name: http
        - containerPort: 22
          protocol: TCP
          name: ssh
        volumeMounts:
        - name: git
          mountPath: "/data"
        - name: config
          mountPath: /data/gitea/conf/app.ini
          subPath: app.ini
      restartPolicy: Always
      terminationGracePeriodSeconds: 20
---
apiVersion: v1
kind: Service
metadata:
  name: gitea
  namespace: ci
  labels:
    name: gitea
spec:
  ports:
    - name: "gitea-http"
      protocol: TCP
      port: 80
      targetPort: 3000
  selector:
    app: gitea
---
apiVersion: v1
kind: Service
metadata:
  name: gitea-ssh
  namespace: ci
  labels:
    name: gitea-ssh
spec:
  type: NodePort
  ports:
    - name: "gitea-ssh"
      protocol: TCP
      port: 22
      nodePort: 22
      targetPort: 22
  selector:
    app: gitea
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: git
  namespace: ci
spec:
  tls:
  - hosts:
    - git.${DOMAIN}
    secretName: git-tls
  rules:
  - host: git.${DOMAIN}
    http:
      paths:
      - backend:
          serviceName: gitea
          servicePort: 80
        path: /
EOF
kubectl apply -f ${TMP}/ci/gitea.yml
git add ${TMP}/ci/gitea.yml

# matchbox
kubectl create secret generic -n ci matchbox-rpc --from-file=ca.crt=ca_chain.pem --from-file=server.crt=matchbox.${DOMAIN}.cert.pem --from-file=server.key=matchbox.${DOMAIN}.key.pem
cat << EOF > ${TMP}/ci/matchbox.yml
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: matchbox
  namespace: ci
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        name: matchbox
        phase: prod
    spec:
      containers:
        - name: matchbox
          image: quay.io/coreos/matchbox:v0.7.1
          env:
            - name: MATCHBOX_ADDRESS
              value: "0.0.0.0:8080"
            - name: MATCHBOX_RPC_ADDRESS
              value: "0.0.0.0:8081"
            - name: MATCHBOX_LOG_LEVEL
              value: "debug"
          ports:
            - name: http
              containerPort: 8080
            - name: https
              containerPort: 8081
          resources:
            requests:
              cpu: "50m"
              memory: "50Mi"
          volumeMounts:
            - name: config
              mountPath: /etc/matchbox
            - name: data
              mountPath: /var/lib/matchbox
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      volumes:
        - name: config
          secret:
            secretName: matchbox-rpc
        - name: data
          hostPath:
            path: /var/lib/matchbox
            type: DirectoryOrCreate
---
apiVersion: v1
kind: Service
metadata:
  name: matchbox
  namespace: ci
spec:
  type: NodePort
  selector:
    name: matchbox
    phase: prod
  ports:
    - name: http
      protocol: TCP
      port: 8080
      nodePort: 8080
      targetPort: 8080
    - name: https
      protocol: TCP
      nodePort: 8081
      port: 8081
      targetPort: 8081
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: dnsmasq
  namespace: ci
  labels:
    app: dnsmasq
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: dnsmasq
    spec:
      containers:
      - name: dnsmasq
        image: graytshirt/dnsmasq
        args:
        - -q
        - -d
        - --dhcp-range=10.202.1.0,10.202.1.254,255.255.254.0,10.202.1.255
        - --enable-tftp
        - --tftp-root=/var/lib/dnsmasq
        - --dhcp-option=6,${IP}
        - --dhcp-match=set:bios,option:client-arch,0
        - --dhcp-boot=tag:bios,undionly.kpxe
        - --dhcp-match=set:efi32,option:client-arch,6
        - --dhcp-boot=tag:efi32,ipxe.efi
        - --dhcp-match=set:efibc,option:client-arch,7
        - --dhcp-boot=tag:efibc,ipxe.efi
        - --dhcp-match=set:efi64,option:client-arch,9
        - --dhcp-boot=tag:efi64,ipxe.efi
        - --dhcp-userclass=set:ipxe,iPXE
        - --dhcp-boot=tag:ipxe,http://matchbox.${DOMAIN}:8080/boot.ipxe
        - --log-queries
        - --log-dhcp
        ports:
        - name: dhcp-server
          containerPort: 67
          protocol: UDP
        - name: dhcp-client
          containerPort: 68
          protocol: UDP
        resources:
          requests:
            cpu: "50m"
            memory: "50Mi"
        volumeMounts:
        - name: data
          mountPath: /var/lib/dnsmasq
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      volumes:
        - name: data
          hostPath:
            path: /var/lib/dnsmasq
            type: DirectoryOrCreate
---
apiVersion: v1
kind: Service
metadata:
  name: dnsmasq
  namespace: ci
  labels:
    app: dnsmasq
spec:
  type: NodePort
  selector:
    app: dnsmasq
  ports:
    - name: dhcp-server
      protocol: UDP
      port: 67
      nodePort: 67
      targetPort: 67
    - name: dhcp-client
      protocol: UDP
      port: 68
      nodePort: 68
      targetPort: 68
EOF
kubectl apply -f ${TMP}/ci/matchbox.yml
git add ${TMP}/ci/matchbox.yml
sudo mkdir -p /var/lib/matchbox/assets

ssh-keygen -t ed25519  -f coreseed_ed25519 -N ''
git add ${TMP}/certs/coreseed_ed25519
git add ${TMP}/certs/coreseed_ed25519.pub

while true; do
	curl -k -f \
		-H 'Content-Type: application/json' \
		-u coreseed:${GITEA_PASS} \
		https://git.${DOMAIN}/api/v1/admin/users/coreseed/keys \
		-d "{\"title\":\"coreseed\",\"read_only\":true,\"key\":\"$(cat coreseed_ed25519.pub)\"}" && break || sleep 1
done
curl -k -f -X POST "https://git.${DOMAIN}/api/v1/admin/users/coreseed/orgs" \
	 -H  "accept: application/json" \
	 -H "content-type: application/json" \
	 -u coreseed:${GITEA_PASS} \
	 -d '{"username":"ci"}'

curl -k -f -X POST "https://git.${DOMAIN}/api/v1/org/ci/repos" \
	 -H "accept: application/json" \
	 -H "content-type: application/json" \
	 -u coreseed:${GITEA_PASS} \
	 -d '{"private":true,"name":"kubernetes", "description": "Automated Kubernetes with Flux"}'

kubectl create secret generic -n ci flux-git-deploy --from-file=identity=coreseed_ed25519
ssh-keyscan  -p 22 -t ssh-ed25519 git.$DOMAIN > known_hosts
sed -i -e "/git.$DOMAIN/d" \
	 -e "/$IP/d" \
	 ~/.ssh/known_hosts
ssh-keyscan  -p 22 -t ssh-ed25519 git.$DOMAIN >> ~/.ssh/known_hosts
ssh-add coreseed_ed25519

kubectl create configmap flux-ssh-config -n ci --from-file=known_hosts
git add known_hosts

git remote add origin git@git.$DOMAIN:ci/kubernetes.git
echo "Auto generated" >> ${TMP}/README.md
git add ${TMP}/README.md
git commit -a -m 'initial commit'
git push -u origin master


cat << EOF > ${TMP}/ci/flux.yml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    name: flux
  name: flux
  namespace: ci
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    name: flux
  name: flux
  namespace: ci
rules:
  - apiGroups: ['*']
    resources: ['*']
    verbs: ['*']
  - nonResourceURLs: ['*']
    verbs: ['*']
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    name: flux
  name: flux
  namespace: ci
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flux
subjects:
  - kind: ServiceAccount
    name: flux
    namespace: ci
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: memcached
  namespace: ci
spec:
  replicas: 1
  selector:
    matchLabels:
      name: memcached
  template:
    metadata:
      labels:
        name: memcached
    spec:
      containers:
      - name: memcached
        image: memcached:1.4.25
        imagePullPolicy: IfNotPresent
        args:
        - -m 64
        - -p 11211
        ports:
        - name: clients
          containerPort: 11211
---
apiVersion: v1
kind: Service
metadata:
  name: memcached
  namespace: ci
spec:
  clusterIP: None
  ports:
    - name: memcached
      port: 11211
  selector:
    name: memcached
  # The memcache client uses DNS to get a list of memcached servers and then
  # uses a consistent hash of the key to determine which server to pick.
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flux
  namespace: ci
spec:
  replicas: 1
  selector:
    matchLabels:
      name: flux
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        prometheus.io.port: "3031" # tell prometheus to scrape /metrics endpoint's port.
      labels:
        name: flux
    spec:
      serviceAccount: flux
      volumes:
      - name: git-key
        secret:
          secretName: flux-git-deploy
          defaultMode: 0400 # when mounted read-only, we won't be able to chmod
      - name: git-keygen
        emptyDir:
          medium: Memory
      # The following volume is for using a customised known_hosts
      # file, which you will need to do if you host your own git
      # repo rather than using github or the like. You'll also need to
      # mount it into the container, below. See
      # https://github.com/weaveworks/flux/blob/master/site/standalone-setup.md#using-a-private-git-host
      - name: ssh-config
        configMap:
          name: flux-ssh-config
      containers:
      - name: flux
        image: quay.io/weaveworks/flux:1.8.1
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
        ports:
        - containerPort: 3030 # informational
        volumeMounts:
        - name: git-key
          mountPath: /etc/fluxd/ssh # to match location given in image's /etc/ssh/config
          readOnly: true # this will be the case perforce in K8s >=1.10
        - name: git-keygen
          mountPath: /var/fluxd/keygen # to match location given in image's /etc/ssh/config
        # Include this if you need to mount a customised known_hosts
        # file; you'll also need the volume declared above.
        - name: ssh-config
          mountPath: /root/.ssh
        args:
        - --ssh-keygen-dir=/var/fluxd/keygen
        - --git-url=git@git.${DOMAIN}:ci/kubernetes.git
        - --git-branch=master
        - --git-poll-interval=120
        - --listen-metrics=:3031
EOF
kubectl apply -f ${TMP}/ci/flux.yml
git add ${TMP}/ci/flux.yml

git commit -a -m 'Adding flux configs'
git push

echo "The coreseed password for Gitea is ${GITEA_PASS}"
